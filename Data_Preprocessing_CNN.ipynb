{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c24c15-7f42-4321-836c-1999f41a1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06068b-d940-415d-9e5f-0114b8dd670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth - Works only with Nvidia Graphics - 2022\n",
    "# Prevent from geeting all GPU RAM's by tensorflow\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abe54c-92ce-429e-98d3-fdd091d8f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1466500-72cd-4b83-925e-47b70e93f811",
   "metadata": {},
   "source": [
    "1.2 Remove all Iamges with wrong extensions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6c269-530e-4e4e-b3a5-aa66595e9fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imghdr\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf340eeb-9493-4f6a-baed-54003707da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001dc82b-1235-41e2-a917-6c4391fc12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_extensions = ['jpeg', 'jpg', 'bmp', 'png'] # List of available extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092fe7aa-c8cd-4724-b533-faf4def41461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for trash data - wrong extensions\n",
    "for image_class in os.listdir(data_dir):\n",
    "    for image in os.listdir(os.path.join(data_dir, image_class)):\n",
    "        image_path = os.path.join(data_dir, image_class, image)\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in image_extensions:\n",
    "                print(f'Image not in extensions list {image_path}')\n",
    "                os.remove(image_path)\n",
    "        except Exception as e:\n",
    "            print(f'Issue with image {image_path}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233598f-967f-4985-a680-4367d3e11952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fb755f-621c-4ac6-95ab-dcef586e4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data - Create datasets with labels (subdirectory names). Split for n-counts of batches with size of 32 (default)\n",
    "data = tf.keras.utils.image_dataset_from_directory('data_v2', image_size=(224,224)) \n",
    "\n",
    "# It is working as Generator - do not loading files into memory\n",
    "# Automaticly resize images - unified size of image 256x256 and create batches of images to maintain better prformance. \n",
    "# If our PC got not as much memory on GPU as it is needed, we can change size of batch ora change size of images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c0582-a802-49a2-9f16-c199c1366741",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.image_dataset_from_directory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8236998f-3055-4df8-9b22-c9ef0593f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = data.as_numpy_iterator() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3affb9fd-20e6-4845-b284-236220f8e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_iterator.next() # This line grab the batches from pipeline - again and again and again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30abe6f4-a07e-485b-a7cf-c455e170da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images represented as numpy arrays\n",
    "len(batch) # Print 2 - one for images in batches in te shape of numpy array\n",
    "batch[0] # Print values from first position in batch\n",
    "batch[0].shape # Show how many images are in a batch - in this case 32 it can be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915b99a-14b2-42ba-8bb3-6ba244447ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1] # Labels of the images taken from directors contained in main directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f0efe-4bf8-4786-b2a4-ef8742ec318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( ncols=4, figsize=(20,20))\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    ax[idx].imshow(img.astype(int))\n",
    "    ax[idx].title.set_text(batch[1][idx])\n",
    "\n",
    "    # Quick check which label is for which picture. In this case 0 - happy, 1 - sad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4aed51-e257-4ade-9906-89625e9fb347",
   "metadata": {},
   "source": [
    "2. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b63da9-e530-4ba0-a39d-c919afcd3b15",
   "metadata": {},
   "source": [
    "2.1 Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf9c16-20fe-4413-8479-c0c9eb7cd831",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda x,y: (x/255, y)) # Allows create transformation in pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe9e70-c015-4950-a1b4-c137a9177099",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_iterator = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a49023-96f3-42cb-b30d-36e7fcca8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = scaled_iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a260a-4620-4a01-9967-240f020b1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832e0ef-1288-48be-bf88-64ddd113d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( ncols=4, figsize=(20,20))\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    ax[idx].imshow(img)\n",
    "    ax[idx].title.set_text(batch[1][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5631f47-f0a2-4e0e-9684-a2baad287df3",
   "metadata": {},
   "source": [
    "2.2 Split Data to Training and Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb66c24-81e8-4fae-8c83-5d3b19a79552",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data) # How many batches have we got. This example: 7 batches with 32 images each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b33284-6a3b-40af-aa64-64d312edd788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split batches between training, validation and testing\n",
    "# The sum of each has to equal the number of batches.\n",
    "train_size = int(len(data)*.7)-2\n",
    "val_size = int(len(data)*.2)+2\n",
    "test_size = int(len(data)*.1)+1\n",
    "\n",
    "# train and val data is used in trianing process. Training data allow us to trian deep learning network, validation is for model checking\n",
    "# test batch is to check the results of training our network -> used at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4092c4d-a776-4b6d-9665-085679a31939",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de178e0-f826-49e4-a01c-c41bec9b1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bae7ae-b799-466d-9346-cc914daef802",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46527e7b-9bf6-45bf-8cb7-3958b9197614",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_size = train_size + val_size + test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425a35e-73e5-46cd-92cf-3328cb86963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b95779-cad8-452c-b6ee-e037f24e2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size)\n",
    "test = data.skip(train_size+val_size).take(test_size)\n",
    "\n",
    "# We allocate the batches of date to further training, validation and testing process. \n",
    "# .take() -> takes applied value of batches.\n",
    "# .skip() -> skip applied value of batche.\n",
    "\n",
    "# Important ! -> batches has to be shuffled before this process, and can not be shuffled after.\n",
    "# It is, because we neeed to keep the order of taken batches to apply skip() function to work\n",
    "# If we wanna shuffle data we has to back to: data = tf.keras.utils.image_dataset_from_directory('data'), and create new, shuffled batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26903989-e63b-4ad2-bc6a-17c8ffde8d48",
   "metadata": {},
   "source": [
    "3. Deep model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb6c55a-b4f3-4608-812c-06303a6ec5c0",
   "metadata": {},
   "source": [
    "3.1 Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d09bf-2f5e-423c-a50f-f98c97f04adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import graphviz\n",
    "import pydot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3955c7d-4ca7-4763-8da5-467e249c79b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc116a1-6599-4d15-98f2-ba79ad7aadb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3 - Convlutinal Layers, 1 - Flatten layer, 2 - Dense layers\n",
    "\n",
    "# Conolutional Layer and MaxPooling Layer\n",
    "# We got: 16 filters catching key features from picture. 3x3 -> 3px by 3px filters size, 1 stride -> moves the 3x3 squer one pixel each time\n",
    "# activation = 'relu' -> the function we use to change input date. Relu changes all negative numbers to 0, \n",
    "# and the positive numbers multipl by a special value. There are many of activation functions. \n",
    "# Non-linear deep learning models are overall better, because deep learning is not linear method\n",
    "# input_shape - size of images go into model. The sizes has been changed earlier in the program\n",
    "\n",
    "model.add(Conv2D(16, (3,3), 1, activation=\"relu\", input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D()) \n",
    "# MaxPoooling2D returns max value, found after activation function. It will condense and scale down the input image. It takes the vlaues from\n",
    "# region. In default (2,2) -> reduce size by half\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3,3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(16, (3,3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "# Number of filters form the channel Value,but we need one chanel value.\n",
    "# Flatten layer Create one-dimensional dataset, which is required input in Dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "# So we got one-dimensional 256 outputs after Dense function, and later we get 1 output.\n",
    "# Dense layers are the fully connected layers in the Keras framework,\n",
    "# We got 256 neurons, and we feed them into Dense layer with relu activation.\n",
    "# After the second Dense operation we expect the value between 0 and 1 -> due to sigmoid chracteristics\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170b0fff-2077-4175-9845-93b4bcff2427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W have t ocompile our model. We use 'adam' optimizer (one of many)\n",
    "model.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be52ad3-377d-4cee-85e0-5bc6a1b52d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() # Allow us to see how the model is converting the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569bd6a4-8d10-475b-9386-614bd1c7e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print network flow chart\n",
    "# tf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59540d7d-3259-4000-9e40-29ddf1544643",
   "metadata": {},
   "source": [
    "3.2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14439746-31b1-40d0-92d2-00ac0d37f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b78886-1079-4851-9c4a-7fc86ba732b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e0272-ac94-49cd-af87-e0325fce6874",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback])\n",
    "# model.fit(): takes training date (dividie earlier in the code), epoches -> how many times we wnat to train our model \n",
    "# -> one epoch is runinng over entire training set of data\n",
    "# the nwe pass our validation vlaues, so we can know how great performance has our model in real-time domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75bdb7-ada7-4a8e-a6f4-f438100a087d",
   "metadata": {},
   "source": [
    "3.3 Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa6e1a-2310-4046-beb3-e9436049d34b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56837f87-ac47-4498-9b39-d0571fddc68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cause we saved information in hist variable, we cna now plot some interesting plots\n",
    "fig = plt.figure()\n",
    "plt.plot(hist.history['loss'], color='teal', label='Train loss')\n",
    "plt.plot(hist.history['val_loss'], color='orange', label='Validation loss')\n",
    "fig.suptitle('Loss', fontsize=15)\n",
    "plt.xlabel('Iteration', fontsize=10)\n",
    "plt.ylabel('Loss value', fontsize=10)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdab40-b068-41ce-90bb-218e1b4ef9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='Train accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='orange', label='Validation accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=15)\n",
    "plt.xlabel('Iteration', fontsize=10)\n",
    "plt.ylabel('Accuracy value', fontsize=10)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9120c35-7f72-4643-a182-1cc51f50c351",
   "metadata": {},
   "source": [
    "4. Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4772cd01-08f9-4feb-b3cd-23358e1f78a5",
   "metadata": {},
   "source": [
    "4.1 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc86768-a737-496d-b311-e7106beb461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3dc11-2174-4bbd-b9f6-e585bd4c324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = Precision()\n",
    "recall = Recall()\n",
    "accuracy = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc583f-23ab-4255-a7ea-bb86057850ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test.as_numpy_iterator():\n",
    "    X, y = batch\n",
    "    ythat = model.predict(X)\n",
    "    precision.update_state(y, ythat)\n",
    "    recall.update_state(y, ythat)\n",
    "    accuracy.update_state(y, ythat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442b0f5-c5ae-4eb1-94c2-0d36c6543182",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Precision: {precision.result().numpy()}, Recall: {recall.result().numpy()}, Accuracy: {accuracy.result().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a526d5c-881e-41b7-ad85-fa4c58a12040",
   "metadata": {},
   "source": [
    "4.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef875ec-b564-4b66-8a20-7c756eeb1723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = cv2.imread('pain_test.JPG')\n",
    "img = cv2.imread('test_Data/neutral_test_v2.JPG')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "fig_3 = plt.figure()\n",
    "plt.imshow(img)\n",
    "fig_3.suptitle('test title', fontsize=20)\n",
    "plt.xlabel('xlabel', fontsize=18)\n",
    "plt.ylabel('ylabel', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740e9ea-5ee6-4233-8476-d32c81bde525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We resized all batches of data to 224x224 size. The input data has to be the same size !!!\n",
    "resize_img = tf.image.resize(img, (224,224))\n",
    "plt.imshow(resize_img.numpy().astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab07e79-7f3f-43c5-a814-f7eb325eaf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(np.expand_dims(resize_img/255, 0)) \n",
    "# The model expect the batch of data not a single image, so we have to wrap data into batch (deeper into the list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b681ae-2044-489f-bd06-af7cfcecff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat\n",
    "#In this case 0 - happy, 1 - sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192fabfd-98f1-4bd3-9db9-3d22fd32a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if yhat < 0.5:\n",
    "    print('We are Normal Normal People, Yuhu :) !!!')\n",
    "else:\n",
    "    print(\"Hello Pain My Old Friend :( \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a09906-a1a1-4a94-8ef2-a8d4c19d7539",
   "metadata": {},
   "source": [
    "5. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f7b19-8be8-439e-9c04-4b1e9a2ae80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097775ca-9af3-4d63-89bf-b23d111fb079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model to file\n",
    "# model.save(os.path.join('models', 'pain_detection_model_v2.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f6c56-77e6-443e-b931-3cf2171d8f90",
   "metadata": {},
   "source": [
    "6. Reuse the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facbba39-fa5e-4df0-993c-eaee0a0650dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from dir\n",
    "new_model = load_model(os.path.join('models', 'pain_detection_model_baseCNN.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1c516-b25e-4b19-a89c-4a6abc9006ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = cv2.imread('test_Data/neutral_test_v1.JPG')\n",
    "input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(input_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b2b162-7fb9-4715-9dd1-86ae5cd4ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_img = tf.image.resize(input_img, (256, 256))\n",
    "plt.imshow(resized_img.numpy().astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86aa50-6325-4db7-99c9-0cccdb678174",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_new = new_model.predict(np.expand_dims(resized_img/255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f26a7-0c15-4090-a74e-a3139b38f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798fd39d-6e0d-4364-8b9e-215639ac2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "if yhat_new < 0.5:\n",
    "    print('We are Normal Normal People, Yuhu :) !!!')\n",
    "    plt.imshow(resized_img.numpy().astype(int))\n",
    "else:\n",
    "    print(\"Hello Pain My Old Friend :( \")\n",
    "    plt.imshow(resized_img.numpy().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b3eefb-c534-4626-943b-6f0f7402f328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
